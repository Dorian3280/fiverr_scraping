{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7ff6ba",
   "metadata": {},
   "source": [
    "# Fiverr Scraping Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c854ae",
   "metadata": {},
   "source": [
    "## Explication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f1ed8",
   "metadata": {},
   "source": [
    "\n",
    "Nous avons décider de partir sur un projet d'analyze de données sur Fiverr, un site où des personnes proposent tout type de services à des utilisateurs. La première étape de ce projet fut de trouver le moyen de récupérer le plus d'informations possibles du site web grâce à la technique de scraping. Le temps de réalisation de cette étape dépend entièrement de l'éthique qu'ont eus les développeurs pour la réalisation dudit site. Et ce jeu de hasard, malheuresement, nous l'avons perdu.\n",
    "\n",
    "![](./images/1.png)\n",
    "\n",
    "on passe de \\<b> à \\<strong> pour aucune raisons pour 2 éléments différents d'une même liste\n",
    "\n",
    "![](./images/2.png)\n",
    "\n",
    "ou encore l'organisation des informations qui est très mal gérée\n",
    "\n",
    "mais bref, juste quelques difficultés incontournables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe25b8e",
   "metadata": {},
   "source": [
    "Nous avons eu 2 idées de script :  \n",
    "    \n",
    "- on choisi un sujet, photoshop, python ou piano, et pour chaque page, on récupères toutes les propositions de service que nous renvoie le site. Chaque page contient environ 40 propositions, et Fiverr nous donne des informations complémentaires comme la note moyenne de l'auteur, le nombre d'avis, la description et le lien du profil. Nous extrayons tout pour analyzer ces informations, pour les mettre en base de données. L'idée est de comparer toutes les propositions entre elles et de retourner la proposition la plus efficace suivant la note et le nombre d'avis.\n",
    "- Depuis la base de données, on scrape tous les profils grâce au lien qu'on a scrapé dans le 1er script. Avec ces informations supplémentaires, nous avons eu l'idée de réaliser un profiler, grâce à un model de machine learning, on lui donne toutes ces informations et ils nous retourne le profil qui à le plus de chance de réussir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a0eff",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244b9943-9be2-48bd-938d-9da591c2765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import unidecode\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import Dash, html, dcc, Input, Output\n",
    "from nltk.tokenize import word_tokenize # Passing the string text into word tokenize for breaking the sentences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18db15",
   "metadata": {},
   "source": [
    "Choix du sujet et création du dossier qui contiendra les fichiers csv, notre base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb9b104-8d3d-4271-b107-b76bdb11a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"python\", \"data science\", \"copyright\", \"comptability\", \"design\", \"excel, javascript\", \"adobe premiere\", \"photoshop\", \" c++\", \"guitare\", \"math\", \"piano\", \"violon\", \"java\"]\n",
    "subject = 'javascript'\n",
    "CARDS_COLUMNS = [\"name\", \"price\", \"nb_comments\", \"note\", \"profil_link\", \"short_description\"]\n",
    "PROFILS_COLUMS = [\"name\", \"location\", \"created_at\", \"response_time\", \"last_order\", \"languages\", \"linked_acc\", \"skills\", \"education\", \"description\"]\n",
    "try: os.mkdir('./data/' + subject)\n",
    "except FileExistsError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca48a4",
   "metadata": {},
   "source": [
    "Cette fonction retourne le driver obligatoire pour récupérer le code html de la page web, les options permettent d'éviter de se faire détecter par les différents bots (les sites web utilisent des techniques pour empêcher le scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('--disable-browser-side-navigation')\n",
    "    return webdriver.Firefox(options=options, executable_path='/home/skyw4rds/Téléchargements/geckodriver-v0.30.0-linux64/geckodriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57aac8e",
   "metadata": {},
   "source": [
    "Le script pour scraper les pages d'un sujet choisi et écrire dans un .csv les informations récupérées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6185063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cards(subject, page=10) -> None:\n",
    "    with open(f'./data/{subject}/cards.csv', 'a') as f:\n",
    "        for p in range(page):\n",
    "            driver = get_driver()\n",
    "            driver.get(f\"https://fr.fiverr.com/search/gigs?query={subject}&source=pagination&search_in=everywhere&search-autocomplete-original-term={subject}&page={p}\")\n",
    "\n",
    "            if 'block.fiverr.com' in driver.current_url or '/404' in driver.current_url:\n",
    "                print('blocked')\n",
    "                driver.close()\n",
    "                raise Exception\n",
    "\n",
    "            elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 'gig-card-layout')]\")\n",
    "            for element in elements:\n",
    "                try: \n",
    "                    name = element.find_element(By.XPATH, \".//div[contains(@class, 'seller-name')]/a\").text\n",
    "                    profil_link = element.find_element(By.XPATH, \".//div[contains(@class, 'seller-name')]/a\").get_attribute('href')\n",
    "                    description = element.find_element(By.XPATH, \".//h3/a\").text.replace(';', ',')\n",
    "                    ratingText = element.find_element(By.XPATH, \".//span[contains(@class, 'gig-rating')]\").text\n",
    "                    groups = re.match(r'(\\d)(?:,)(\\d)(?:\\()(\\d+)(?:\\))', ratingText).groups()\n",
    "                    note = int(groups[0])+int(groups[1])/10\n",
    "                    price = int(element.find_element(By.XPATH, \".//a[contains(@class, 'price')]/span\").text[:-2])/100\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"{name} -> {str(e)}\")\n",
    "                    driver.close()\n",
    "                        \n",
    "                f.write(f\"{name};{price};{int(groups[2])};{note};{profil_link};{description}\\n\")\n",
    "                \n",
    "            driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3a2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cards = scrape_cards(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28cb12",
   "metadata": {},
   "source": [
    "On récupère ces informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f8bb88-0e5e-4a69-9c59-cf58d220dad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards = pd.read_csv(f'./data/{subject}/cards.csv', sep=';', names=CARDS_COLUMNS) \\\n",
    "    .drop_duplicates(subset=['short_description', 'name'])\n",
    "cards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77262747",
   "metadata": {},
   "source": [
    "On scrape tous les profils en récupérant les liens des profils dans cards.csv  \n",
    "A cause de la detection de scraping, il est difficile de tout scraper d'un coup, cette fonction permet de fragmenter le scraping en plusieurs fois, dynamiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069fc080-450a-486e-8dc9-5ada4126bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_index():\n",
    "    with open(f'./data/{subject}/profils.csv', 'ab+') as file:\n",
    "        if file.tell() == 0:\n",
    "            return 0\n",
    "        while file.read(1) != b'\\n':\n",
    "            try:\n",
    "                file.seek(-1, os.SEEK_CUR)\n",
    "                file.seek(-1, os.SEEK_CUR)\n",
    "            except OSError:\n",
    "                break\n",
    "        last_profil_name = file.readline().decode('utf-8').split(';')[0]\n",
    "        \n",
    "    return cards.index[cards[\"name\"] == last_profil_name][0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c782f39",
   "metadata": {},
   "source": [
    "Le script pour scraper tous les profils et écrire dans un .csv leurs informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6c655f-ae97-4466-95f5-2822127d2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_profils(subject, start, end):\n",
    "    with open(f'./data/{subject}/profils.csv', 'a') as f:\n",
    "            \n",
    "        for id in range(start, end):\n",
    "            \n",
    "            driver = get_driver()\n",
    "            driver.get(cards.loc[id]['profil_link'])\n",
    "            \n",
    "            # The website can block from request or the page doesn't exist or the profil link doesn't exist so it returns to the main page\n",
    "            if 'block.fiverr.com' in driver.current_url:\n",
    "                driver.close()\n",
    "                raise Exception\n",
    "                \n",
    "            name = cards.loc[id]['name']\n",
    "            f.write(f\"{name}\")\n",
    "            \n",
    "            if  '/404' in driver.current_url or len(driver.current_url) < 25:\n",
    "                f.write(f\"\\n\")\n",
    "                continue\n",
    "\n",
    "            stats = [i.text.strip() for i in driver.find_elements(By.XPATH, \"//ul[contains(@class, 'user-stats')]/li/*[self::b or self::strong]\")]\n",
    "            location = stats[0]\n",
    "            created_at = stats[1]\n",
    "            response_time = stats[2]\n",
    "            last_order = stats[3]\n",
    "            languages = [f\"{i.text[i.text.find('(')+1:i.text.find(')')]}-{i.text.split('-')[-1].strip()}\" for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'languages')]/ul/li\")]\n",
    "            linked_acc = [i.text for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'linked-accounts')]/ul/li/span[@class='text']\")]\n",
    "            skills = [i.text for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'skills')]/ul/li/a\")]\n",
    "            \n",
    "            try:\n",
    "                education = driver.find_element(By.XPATH, \"//div[contains(@class, 'education-list')]/ul/li/p\").text\n",
    "            except Exception:\n",
    "                education = ''\n",
    "            description = driver.find_element(By.XPATH, \"//div[contains(@class, 'description')]/p\").text.replace(';', ',')\n",
    "            \n",
    "            f.write(f\";{location};{created_at};{response_time};{last_order};{'|'.join(languages)};{'|'.join(linked_acc)};{'|'.join(skills)};{education};{description}\\n\")\n",
    "            time.sleep(5)\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e24632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = get_start_index()\n",
    "# end = start + 100\n",
    "# scrape_profils(subject, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf203533",
   "metadata": {},
   "source": [
    "Formatage du Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97099ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_CORRESPONDING = {\n",
    "    'janv.': '01',\n",
    "    'févr.': '02',\n",
    "    'mars': '03',\n",
    "    'avr.': '04',\n",
    "    'mai': '05',\n",
    "    'juin':'06',\n",
    "    'juil.': '07',\n",
    "    'août': '08',\n",
    "    'sept.': '09',\n",
    "    'oct.': '10',\n",
    "    'nov.': '11',\n",
    "    'déc.': '12',\n",
    "}\n",
    "regex = re.compile('(\\w+.?) (\\d+)')\n",
    "\n",
    "def sigmoid(x,weight=1):\n",
    "    return 1 / (1 + math.exp(-x/weight))\n",
    "\n",
    "filtre_stopfr = lambda text: [token for token in text if token.lower() not in set(stopwords.words('french'))]\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f897ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossessing(row: pd.Series):\n",
    "    \n",
    "    ## FROM skills TO skill_1, skill_2, ...\n",
    "    try:\n",
    "        skills = row[\"skills\"].split(\"|\")\n",
    "    except AttributeError:\n",
    "        skills = []\n",
    "    \n",
    "    for i, skill in enumerate(skills):\n",
    "        row[f\"skill_{i+1}\"] = skill\n",
    "    \n",
    "    ## FROM linked_accs TO linked_acc_1, linked_acc_2, ...\n",
    "    try:\n",
    "        linked_accs = row[\"linked_acc\"].split(\"|\")\n",
    "    except AttributeError:\n",
    "        linked_accs = []\n",
    "    \n",
    "    for i, linked_acc in enumerate(linked_accs):\n",
    "        row[f\"linked_acc_{i+1}\"] = linked_acc\n",
    "    \n",
    "    ## FROM languages TO language_1, language_2, ...\n",
    "    ## ADD level_1, level_2, ...\n",
    "    try:\n",
    "        languages = row[\"languages\"].split(\"|\")\n",
    "    except AttributeError:\n",
    "        languages = []\n",
    "        \n",
    "    languages = [tuple(language.split('-', 1)) for language in languages]\n",
    "    \n",
    "    for i, (language, level) in enumerate(languages):\n",
    "        if '/' in level: level = level.split('/')[0].strip()\n",
    "        row[f\"language_{i+1}\"] = language\n",
    "        row[f\"level_{i+1}\"] = level\n",
    "\n",
    "    ## FROM \"mai-2005\" TO 2005/05/01\n",
    "    row[\"created_at\"] = re.sub(regex, lambda x: f\"{x.group(2)}-{MONTH_CORRESPONDING[x.group(1)]}\", row[\"created_at\"])\n",
    "        \n",
    "    ## FROM response_time \"3 heures\" TO response_time_day, response_time_hour\n",
    "    response_time = re.match(r'(\\d+) (\\w+)', row[\"response_time\"]).groups()\n",
    "    if 'jour' in response_time[1]:\n",
    "        row[\"response_time_day\"] = response_time[0]\n",
    "        row[\"response_time_hour\"] = 0\n",
    "    else:\n",
    "        row[\"response_time_day\"] = 0\n",
    "        row[\"response_time_hour\"] = response_time[0]\n",
    "    \n",
    "    ## FROM last_order \"3 jours\" TO last_order_day, last_order_hour\n",
    "    last_order = re.match(r'(\\d+) (\\w+)', row[\"last_order\"]).groups()\n",
    "    if 'jour' in last_order[1]:\n",
    "        row[\"last_order_day\"] = last_order[0]\n",
    "        row[\"last_order_hour\"] = 0\n",
    "    else:\n",
    "        row[\"last_order_day\"] = 0\n",
    "        row[\"last_order_hour\"] = last_order[0]\n",
    "    \n",
    "    ## ADD score \n",
    "    row['score'] = row['note'] * sigmoid(row['nb_comments'], 50)\n",
    "    \n",
    "    ## Drop columns\n",
    "    row.drop([\"languages\", \"linked_acc\", \"skills\", \"response_time\", \"last_order\"], inplace=True)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ea26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "    df[\"last_order_day\"] = df[\"last_order_day\"].astype('int')\n",
    "    df[\"last_order_hour\"] = df[\"last_order_hour\"].astype('int')\n",
    "    df[\"response_time_hour\"] = df[\"response_time_hour\"].astype('int')\n",
    "    df[\"response_time_day\"] = df[\"response_time_day\"].astype('int')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23883186",
   "metadata": {},
   "outputs": [],
   "source": [
    "profils = pd.read_csv(f'./data/{subject}/profils.csv', sep=';', names=PROFILS_COLUMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630f0ab",
   "metadata": {},
   "source": [
    "Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a2cf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_skill(df: pd.DataFrame):\n",
    "    skills = Counter()\n",
    "    for rowIndex, row in df.filter(regex=r'skill_\\d+').iterrows():\n",
    "        for columnIndex, value in row.items():\n",
    "            if not pd.isnull(value): skills[value] += 1\n",
    "\n",
    "    return skills.most_common(10)\n",
    "\n",
    "def handle_languages(df: pd.DataFrame):\n",
    "    languages = Counter()\n",
    "    for rowIndex, row in df.filter(regex=r'language_\\d+').iterrows():\n",
    "        for columnIndex, value in row.items():\n",
    "            if not pd.isnull(value): languages[value] += 1\n",
    "\n",
    "    return languages.most_common(10)\n",
    "\n",
    "def handle_note(df: pd.DataFrame):\n",
    "    print(pd.cut(df['note'], bins=np.linspace(0, 5, num=50)))\n",
    "\n",
    "def get_best(df: pd.DataFrame, nb: int) -> pd.DataFrame:\n",
    "    return df.sort_values(by='score', ascending=False).iloc[:nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16669e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = FrenchStemmer()\n",
    "\n",
    "    # remove accents\n",
    "    text = unidecode.unidecode(str(text))\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = \" \".join(tokenizer.tokenize(text))\n",
    "    \n",
    "    # remove useless words\n",
    "    words = filtre_stopfr(word_tokenize(text, language=\"french\"))\n",
    "    \n",
    "    # normalize words\n",
    "    words = \" \".join([stemmer.stem(w) for w in words])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5658b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_div(df: pd.DataFrame, category: str):\n",
    "\n",
    "    skills = handle_skill(df)\n",
    "    languages = handle_languages(df)\n",
    "    profils = get_best(df, 3)\n",
    "\n",
    "    category_title = html.H2(category.title(), style={'font-size': '1.7rem', 'text-align': 'center'})\n",
    "\n",
    "    best_profils_title = html.H3('💪 Meilleurs profils 💪', style={'font-size': '1.5rem', 'text-align': 'center'})\n",
    "\n",
    "    graph_skills = html.Div(dcc.Graph(\n",
    "        figure=px.bar(df, x=[i[0] for i in skills], y=[i[1] for i in skills], barmode=\"group\", title='Compétences', labels={'x': '', 'y': ''})\n",
    "    ))\n",
    "\n",
    "    graph_languages = html.Div(dcc.Graph(\n",
    "        figure=px.bar(df, x=[i[0] for i in languages], y=[i[1] for i in languages], barmode=\"group\", title='Langues', labels={'x': '', 'y': ''})\n",
    "    ))\n",
    "\n",
    "\n",
    "    txt = normalize_text(' '.join(df['description'].to_list()))\n",
    "    wordcloud.generate(txt)\n",
    "    wordcloud.to_file(filename='images/words.png')\n",
    "    \n",
    "    encoded_image = base64.b64encode(open('images/words.png', 'rb').read())\n",
    "    image_words = html.Div(html.Img(src=f'data:image/png;base64,{encoded_image.decode()}'), style={'margin': '20px 0'})\n",
    "\n",
    "    profils_div = []\n",
    "    for index, profil in profils.iterrows():\n",
    "        \n",
    "        div = html.Div([\n",
    "            html.Span(profil['name'], style={'font-size': '1.2rem'}),\n",
    "            html.Span(f\"⭐ {profil['note']} ({profil['nb_comments']})\", style={'margin': '5px 0'}),\n",
    "            html.Span(f\"🏆 Score : {round(profil['score'], 3)}\", style={'margin': '5px 0'}),\n",
    "            html.Span(profil['description']),\n",
    "            html.A('Lien du profil', href=profil['profil_link'], style={'font-size': '1.2rem'})\n",
    "        ], style={'margin': '0 20px', 'padding': '10px', 'border': '1px solid white', 'width': '400px', 'display': 'flex', 'flex-direction': 'column', 'align-items': 'center'})\n",
    "\n",
    "        profils_div.append(div)\n",
    "\n",
    "    profils_render = html.Div(profils_div, style={'display': 'flex'})\n",
    "\n",
    "    return html.Div(children=[\n",
    "        category_title,\n",
    "        best_profils_title,\n",
    "        profils_render,\n",
    "        image_words,\n",
    "        graph_skills,\n",
    "        graph_languages,\n",
    "    ], style={'color': 'white', 'display': 'flex', 'flex-direction': 'column', 'align-items': 'center'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40016d",
   "metadata": {},
   "source": [
    "Je crée un serveur Dash pour visualiser certaines informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f07bf5a5-14ac-43e1-b08a-eed019efa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309d2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['javascript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8472d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for category in categories:\n",
    "\n",
    "    profils = pd.read_csv(f'./data/{category}/profils.csv', sep=';', names=PROFILS_COLUMS)\n",
    "    \n",
    "    new_profils = (profils\n",
    "        .merge(cards, how='left', on=\"name\") \n",
    "        .apply(preprossessing, axis=1) \n",
    "    )\n",
    "    new_profils = convert_type_column(new_profils)\n",
    "\n",
    "    dfs.append(new_profils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da77228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      (4.898, 5.0]\n",
      "1      (4.898, 5.0]\n",
      "2      (4.898, 5.0]\n",
      "3      (4.898, 5.0]\n",
      "4      (4.898, 5.0]\n",
      "5      (4.898, 5.0]\n",
      "6      (4.898, 5.0]\n",
      "7      (4.898, 5.0]\n",
      "8      (4.898, 5.0]\n",
      "9      (4.898, 5.0]\n",
      "10     (4.898, 5.0]\n",
      "11     (4.898, 5.0]\n",
      "12     (4.898, 5.0]\n",
      "13     (4.898, 5.0]\n",
      "14     (4.898, 5.0]\n",
      "15     (4.898, 5.0]\n",
      "16     (4.898, 5.0]\n",
      "17     (4.898, 5.0]\n",
      "18     (4.898, 5.0]\n",
      "19     (4.898, 5.0]\n",
      "20     (4.898, 5.0]\n",
      "21     (4.898, 5.0]\n",
      "22     (4.898, 5.0]\n",
      "23     (4.898, 5.0]\n",
      "24     (4.898, 5.0]\n",
      "25     (4.898, 5.0]\n",
      "26     (4.898, 5.0]\n",
      "27     (4.898, 5.0]\n",
      "28     (4.898, 5.0]\n",
      "29     (4.898, 5.0]\n",
      "30     (4.898, 5.0]\n",
      "31     (4.898, 5.0]\n",
      "32    (4.388, 4.49]\n",
      "33     (4.898, 5.0]\n",
      "34     (4.898, 5.0]\n",
      "35     (4.898, 5.0]\n",
      "36     (4.898, 5.0]\n",
      "37     (4.898, 5.0]\n",
      "38     (4.898, 5.0]\n",
      "39     (4.898, 5.0]\n",
      "40     (4.898, 5.0]\n",
      "41     (4.898, 5.0]\n",
      "42     (4.898, 5.0]\n",
      "43     (4.898, 5.0]\n",
      "44     (4.898, 5.0]\n",
      "45     (4.898, 5.0]\n",
      "46     (4.898, 5.0]\n",
      "47     (4.898, 5.0]\n",
      "48     (4.898, 5.0]\n",
      "Name: note, dtype: category\n",
      "Categories (49, interval[float64, right]): [(0.0, 0.102] < (0.102, 0.204] < (0.204, 0.306] < (0.306, 0.408] ... (4.592, 4.694] < (4.694, 4.796] < (4.796, 4.898] < (4.898, 5.0]]\n"
     ]
    }
   ],
   "source": [
    "handle_note(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9d34b37-4e4a-41b6-a67b-e0e8bc059759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     NaN\n",
      "1     NaN\n",
      "2     NaN\n",
      "3     NaN\n",
      "4     NaN\n",
      "5     NaN\n",
      "6     NaN\n",
      "7     NaN\n",
      "8     NaN\n",
      "9     NaN\n",
      "10    NaN\n",
      "11    NaN\n",
      "12    NaN\n",
      "13    NaN\n",
      "14    NaN\n",
      "15    NaN\n",
      "16    NaN\n",
      "17    NaN\n",
      "18    NaN\n",
      "19    NaN\n",
      "20    NaN\n",
      "21    NaN\n",
      "22    NaN\n",
      "23    NaN\n",
      "24    NaN\n",
      "25    NaN\n",
      "26    NaN\n",
      "27    NaN\n",
      "28    NaN\n",
      "29    NaN\n",
      "30    NaN\n",
      "31    NaN\n",
      "32    NaN\n",
      "33    NaN\n",
      "34    NaN\n",
      "35    NaN\n",
      "36    NaN\n",
      "37    NaN\n",
      "38    NaN\n",
      "39    NaN\n",
      "40    NaN\n",
      "41    NaN\n",
      "42    NaN\n",
      "43    NaN\n",
      "44    NaN\n",
      "45    NaN\n",
      "46    NaN\n",
      "47    NaN\n",
      "48    NaN\n",
      "Name: note, dtype: category\n",
      "Categories (0, interval[int64, right]): []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7f2676cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @app.callback(\n",
    "#     Output('dd-output-container', 'children'),\n",
    "#     Input('demo-dropdown', 'value'),\n",
    "# )\n",
    "# def update_output(value: str):\n",
    "#     return f'{value.title()}'\n",
    "\n",
    "app.layout = html.Div(children=[get_div(df, category) for df, category in zip(dfs, categories)])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(\n",
    "        debug=True,\n",
    "        mode='inline'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
