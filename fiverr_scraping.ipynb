{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7ff6ba",
   "metadata": {},
   "source": [
    "# Fiverr Scraping Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c854ae",
   "metadata": {},
   "source": [
    "## Explication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f1ed8",
   "metadata": {},
   "source": [
    "\n",
    "Nous avons décider de partir sur un projet d'analyze de données sur Fiverr, un site où des personnes proposent tout type de services à des utilisateurs. La première étape de ce projet fut de trouver le moyen de récupérer le plus d'informations possibles du site web grâce à la technique de scraping. Le temps de réalisation de cette étape dépend entièrement de l'éthique qu'ont eus les développeurs pour la réalisation dudit site. Et ce jeu de hasard, malheuresement, nous l'avons perdu.\n",
    "\n",
    "![](./images/1.png)\n",
    "\n",
    "on passe de \\<b> à \\<strong> pour aucune raisons pour 2 éléments différents d'une même liste\n",
    "\n",
    "![](./images/2.png)\n",
    "\n",
    "ou encore l'organisation des informations qui est très mal gérée\n",
    "\n",
    "mais bref, juste quelques difficultés incontournables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe25b8e",
   "metadata": {},
   "source": [
    "Nous avons eu 2 idées de script :  \n",
    "    \n",
    "- on choisi un sujet, photoshop, python ou piano, et pour chaque page, on récupères toutes les propositions de service que nous renvoie le site. Chaque page contient environ 40 propositions, et Fiverr nous donne des informations complémentaires comme la note moyenne de l'auteur, le nombre d'avis, la description et le lien du profil. Nous extrayons tout pour analyzer ces informations, pour les mettre en base de données. L'idée est de comparer toutes les propositions entre elles et de retourner la proposition la plus efficace suivant la note et le nombre d'avis.\n",
    "- Depuis la base de données, on scrape tous les profils grâce au lien qu'on a scrapé dans le 1er script. Avec ces informations supplémentaires, nous avons eu l'idée de réaliser un profiler, grâce à un model de machine learning, on lui donne toutes ces informations et ils nous retourne le profil qui à le plus de chance de réussir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a0eff",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b9943-9be2-48bd-938d-9da591c2765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import jellyfish\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18db15",
   "metadata": {},
   "source": [
    "Choix du sujet et création du dossier qui contiendra les fichiers csv, notre base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9b104-8d3d-4271-b107-b76bdb11a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"python\", \"data science\", \"copyright\", \"comptability\", \"design\", \"excel, javascript\", \"adobe premiere\", \"photoshop\", \" c++\", \"guitare\", \"math\", \"piano\", \"violon\", \"java\"]\n",
    "subject = 'photoshop'\n",
    "\n",
    "try: os.mkdir('./data/' + subject)\n",
    "except FileExistsError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca48a4",
   "metadata": {},
   "source": [
    "Cette fonction retourne le driver obligatoire pour récuperer le code html de la page web, les options permettent d'éviter de se faire détecter par les différents bots (les sites web utilisent des techniques pour empêcher le scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('--disable-browser-side-navigation')\n",
    "    return webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57aac8e",
   "metadata": {},
   "source": [
    "Le script pour scraper les pages d'un sujet choisi et écrire dans un .csv les informations récupérées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6185063",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/{subject}/cards.csv', 'a') as f:\n",
    "    if f.tell() == 0:\n",
    "        f.write(\"name;price;nb_comments;note;profil_link;description\")\n",
    "    for p in range(10):\n",
    "        driver = get_driver()\n",
    "        driver.get(f\"https://fr.fiverr.com/search/gigs?query={subject}&source=pagination&search_in=everywhere&search-autocomplete-original-term={subject}&page={p}\")\n",
    "\n",
    "        if 'block.fiverr.com' in driver.current_url or '/404' in driver.current_url:\n",
    "            print('blocked')\n",
    "            raise Exception\n",
    "\n",
    "        elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 'gig-card-layout')]\")\n",
    "        for i, element in enumerate(elements):\n",
    "            try: \n",
    "                name = element.find_element(By.XPATH, \".//div[contains(@class, 'seller-name')]/a\").text\n",
    "                profil_link = element.find_element(By.XPATH, \".//div[contains(@class, 'seller-name')]/a\").get_attribute('href')\n",
    "                description = element.find_element(By.XPATH, \".//h3/a\").text.replace(';', ',')\n",
    "                ratingText = element.find_element(By.XPATH, \".//span[contains(@class, 'gig-rating')]\").text\n",
    "                groups = re.match(r'(\\d)(?:,)(\\d)(?:\\()(\\d+)(?:\\))', ratingText).groups()\n",
    "                note = int(groups[0])+int(groups[1])/10\n",
    "                price = int(element.find_element(By.XPATH, \".//a[contains(@class, 'price')]/span\").text[:-2])/100\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(name)\n",
    "                    \n",
    "            f.write(f\"{name};{price};{int(groups[2])};{note};{profil_link};{description}\\n\")\n",
    "            \n",
    "        driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28cb12",
   "metadata": {},
   "source": [
    "On récupère ces informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f8bb88-0e5e-4a69-9c59-cf58d220dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = pd.read_csv(f'./data/{subject}/cards.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c782f39",
   "metadata": {},
   "source": [
    "Le script pour scraper tous les profils et écrire dans un .csv toutes leurs informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c655f-ae97-4466-95f5-2822127d2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/{subject}/profils.csv', 'a') as f:\n",
    "    if f.tell() == 0:\n",
    "        f.write(f\"name;location;created_at;response_time;last_order:languages;linked_acc;skills;education;description\\n\")\n",
    "\n",
    "    for id in range(30):\n",
    "        driver = get_driver()\n",
    "        driver.get(cards.loc[id]['profil_link'])\n",
    "        # The website can block from request or the page doesn't exist or the profil link doesn't exist so it returns to the main page\n",
    "        if 'block.fiverr.com' in driver.current_url or '/404' in driver.current_url or len(driver.current_url) < 25:\n",
    "            raise Exception\n",
    "\n",
    "        name = cards.loc[id]['name']\n",
    "        stats = [i.text.strip() for i in driver.find_elements(By.XPATH, \"//ul[contains(@class, 'user-stats')]/li/*[self::b or self::strong]\")]\n",
    "        location = stats[0]\n",
    "        created_at = stats[1]\n",
    "        response_time = stats[2]\n",
    "        last_order = stats[3]\n",
    "        languages = [f\"{i.text[i.text.find('(')+1:i.text.find(')')]}-{i.text.split('-')[-1].strip()}\" for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'languages')]/ul/li\")]\n",
    "        linked_acc = [i.text for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'linked-accounts')]/ul/li/span[@class='text']\")]\n",
    "        skills = [i.text for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'skills')]/ul/li/a\")]\n",
    "        try:\n",
    "            education = driver.find_element(By.XPATH, \"//div[contains(@class, 'education-list')]/ul/li/p\").text\n",
    "        except Exception:\n",
    "            education = ''\n",
    "        description = driver.find_element(By.XPATH, \"//div[contains(@class, 'description')]/p\").text.replace(';', ',')\n",
    "        \n",
    "        f.write(f\"{name};{location};{created_at};{response_time};{last_order};{'|'.join(languages)};{'|'.join(linked_acc)};{'|'.join(skills)};{education};{description}\\n\")\n",
    "        \n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a971d1e",
   "metadata": {},
   "source": [
    "On récupère ces informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c6317-a6a3-41c1-b7c9-c9016c692d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "profils = pd.read_csv(f'./data/{subject}/profils.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d086a0",
   "metadata": {},
   "source": [
    "Cette function permet de compter toutes les skills de tous les profils pour savoir quels compétences sont les plus demandées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af258a-c400-42b2-97f9-ef3719cdf11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_skills(l: list, nb: int):\n",
    "    skills = Counter({})\n",
    "    for row in l:\n",
    "        temp = row.split('|')\n",
    "        for i in temp:\n",
    "            skills[i] += 1\n",
    "    return skills.most_common(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bf5a5-14ac-43e1-b08a-eed019efa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_common_skills(list(profils['skills']), 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
