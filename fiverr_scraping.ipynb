{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7ff6ba",
   "metadata": {},
   "source": [
    "# Fiverr Scraping Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c854ae",
   "metadata": {},
   "source": [
    "## Explication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f1ed8",
   "metadata": {},
   "source": [
    "\n",
    "Nous avons d√©cider de partir sur un projet d'analyze de donn√©es sur Fiverr, un site o√π des personnes proposent tout type de services √† des utilisateurs. La premi√®re √©tape de ce projet fut de trouver le moyen de r√©cup√©rer le plus d'informations possibles du site web gr√¢ce √† la technique de scraping. Le temps de r√©alisation de cette √©tape d√©pend enti√®rement de l'√©thique qu'ont eus les d√©veloppeurs pour la r√©alisation dudit site. Et ce jeu de hasard, malheuresement, nous l'avons perdu.\n",
    "\n",
    "![](./images/1.png)\n",
    "\n",
    "on passe de \\<b> √† \\<strong> pour aucune raisons pour 2 √©l√©ments diff√©rents d'une m√™me liste\n",
    "\n",
    "![](./images/2.png)\n",
    "\n",
    "ou encore l'organisation des informations qui est tr√®s mal g√©r√©e\n",
    "\n",
    "mais bref, juste quelques difficult√©s incontournables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe25b8e",
   "metadata": {},
   "source": [
    "Nous avons eu 2 id√©es de script :  \n",
    "    \n",
    "- on choisi un sujet, photoshop, python ou piano, et pour chaque page, on r√©cup√®res toutes les propositions de service que nous renvoie le site. Chaque page contient environ 40 propositions, et Fiverr nous donne des informations compl√©mentaires comme la note moyenne de l'auteur, le nombre d'avis, la description et le lien du profil. Nous extrayons tout pour analyzer ces informations, pour les mettre en base de donn√©es. L'id√©e est de comparer toutes les propositions entre elles et de retourner la proposition la plus efficace suivant la note et le nombre d'avis.\n",
    "- Depuis la base de donn√©es, on scrape tous les profils gr√¢ce au lien qu'on a scrap√© dans le 1er script. Avec ces informations suppl√©mentaires, nous avons eu l'id√©e de r√©aliser un profiler, gr√¢ce √† un model de machine learning, on lui donne toutes ces informations et ils nous retourne le profil qui √† le plus de chance de r√©ussir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a0eff",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244b9943-9be2-48bd-938d-9da591c2765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import unidecode\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import Dash, html, dcc, Input, Output\n",
    "from nltk.tokenize import word_tokenize # Passing the string text into word tokenize for breaking the sentences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18db15",
   "metadata": {},
   "source": [
    "Choix du sujet et cr√©ation du dossier qui contiendra les fichiers csv, notre base de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb9b104-8d3d-4271-b107-b76bdb11a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"python\", \"data science\", \"copyright\", \"comptability\", \"design\", \"excel, javascript\", \"adobe premiere\", \"photoshop\", \" c++\", \"guitare\", \"math\", \"piano\", \"violon\", \"java\"]\n",
    "subject = 'javascript'\n",
    "CARDS_COLUMNS = [\"name\", \"price\", \"nb_comments\", \"note\", \"profil_link\", \"short_description\"]\n",
    "PROFILS_COLUMS = [\"name\", \"location\", \"created_at\", \"response_time\", \"last_order\", \"languages\", \"linked_acc\", \"skills\", \"education\", \"description\"]\n",
    "try: os.mkdir('./data/' + subject)\n",
    "except FileExistsError: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca48a4",
   "metadata": {},
   "source": [
    "Cette fonction retourne le driver obligatoire pour r√©cup√©rer le code html de la page web, les options permettent d'√©viter de se faire d√©tecter par les diff√©rents bots (les sites web utilisent des techniques pour emp√™cher le scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754f4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('--disable-browser-side-navigation')\n",
    "    return webdriver.Firefox(options=options, executable_path='/home/skyw4rds/T√©l√©chargements/geckodriver-v0.30.0-linux64/geckodriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57aac8e",
   "metadata": {},
   "source": [
    "Le script pour scraper les pages d'un sujet choisi et √©crire dans un .csv les informations r√©cup√©r√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6185063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cards(subject, page=10) -> None:\n",
    "    with open(f'./data/{subject}/cards.csv', 'a') as f:\n",
    "        for p in range(page):\n",
    "            driver = get_driver()\n",
    "            driver.get(f\"https://fr.fiverr.com/search/gigs?query={subject}&source=pagination&search_in=everywhere&search-autocomplete-original-term={subject}&page={p}\")\n",
    "\n",
    "            if 'block.fiverr.com' in driver.current_url or '/404' in driver.current_url:\n",
    "                print('blocked')\n",
    "                driver.close()\n",
    "                raise Exception\n",
    "\n",
    "            elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 'gig-card-layout')]\")\n",
    "            for element in elements:\n",
    "                try: \n",
    "                    name = element.find_element(By.XPATH, \".//div[contains(@class, 'seller-name')]/a\").text\n",
    "                    profil_link = element.find_element(By.XPATH, \".//div[contains(@class, 'seller-name')]/a\").get_attribute('href')\n",
    "                    description = element.find_element(By.XPATH, \".//h3/a\").text.replace(';', ',')\n",
    "                    ratingText = element.find_element(By.XPATH, \".//span[contains(@class, 'gig-rating')]\").text\n",
    "                    groups = re.match(r'(\\d)(?:,)(\\d)(?:\\()(\\d+)(?:\\))', ratingText).groups()\n",
    "                    note = int(groups[0])+int(groups[1])/10\n",
    "                    price = int(element.find_element(By.XPATH, \".//a[contains(@class, 'price')]/span\").text[:-2])/100\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"{name} -> {str(e)}\")\n",
    "                    driver.close()\n",
    "                        \n",
    "                f.write(f\"{name};{price};{int(groups[2])};{note};{profil_link};{description}\\n\")\n",
    "                \n",
    "            driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3a2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cards = scrape_cards(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28cb12",
   "metadata": {},
   "source": [
    "On r√©cup√®re ces informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f8bb88-0e5e-4a69-9c59-cf58d220dad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards = pd.read_csv(f'./data/{subject}/cards.csv', sep=';', names=CARDS_COLUMNS) \\\n",
    "    .drop_duplicates(subset=['short_description', 'name'])\n",
    "cards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77262747",
   "metadata": {},
   "source": [
    "On scrape tous les profils en r√©cup√©rant les liens des profils dans cards.csv  \n",
    "A cause de la detection de scraping, il est difficile de tout scraper d'un coup, cette fonction permet de fragmenter le scraping en plusieurs fois, dynamiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069fc080-450a-486e-8dc9-5ada4126bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_index():\n",
    "    with open(f'./data/{subject}/profils.csv', 'ab+') as file:\n",
    "        if file.tell() == 0:\n",
    "            return 0\n",
    "        while file.read(1) != b'\\n':\n",
    "            try:\n",
    "                file.seek(-1, os.SEEK_CUR)\n",
    "                file.seek(-1, os.SEEK_CUR)\n",
    "            except OSError:\n",
    "                break\n",
    "        last_profil_name = file.readline().decode('utf-8').split(';')[0]\n",
    "        \n",
    "    return cards.index[cards[\"name\"] == last_profil_name][0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c782f39",
   "metadata": {},
   "source": [
    "Le script pour scraper tous les profils et √©crire dans un .csv leurs informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6c655f-ae97-4466-95f5-2822127d2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_profils(subject, start, end):\n",
    "    with open(f'./data/{subject}/profils.csv', 'a') as f:\n",
    "            \n",
    "        for id in range(start, end):\n",
    "            \n",
    "            driver = get_driver()\n",
    "            driver.get(cards.loc[id]['profil_link'])\n",
    "            \n",
    "            # The website can block from request or the page doesn't exist or the profil link doesn't exist so it returns to the main page\n",
    "            if 'block.fiverr.com' in driver.current_url:\n",
    "                driver.close()\n",
    "                raise Exception\n",
    "                \n",
    "            name = cards.loc[id]['name']\n",
    "            f.write(f\"{name}\")\n",
    "            \n",
    "            if  '/404' in driver.current_url or len(driver.current_url) < 25:\n",
    "                f.write(f\"\\n\")\n",
    "                continue\n",
    "\n",
    "            stats = [i.text.strip() for i in driver.find_elements(By.XPATH, \"//ul[contains(@class, 'user-stats')]/li/*[self::b or self::strong]\")]\n",
    "            location = stats[0]\n",
    "            created_at = stats[1]\n",
    "            response_time = stats[2]\n",
    "            last_order = stats[3]\n",
    "            languages = [f\"{i.text[i.text.find('(')+1:i.text.find(')')]}-{i.text.split('-')[-1].strip()}\" for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'languages')]/ul/li\")]\n",
    "            linked_acc = [i.text for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'linked-accounts')]/ul/li/span[@class='text']\")]\n",
    "            skills = [i.text for i in driver.find_elements(By.XPATH, \"//div[contains(@class, 'skills')]/ul/li/a\")]\n",
    "            \n",
    "            try:\n",
    "                education = driver.find_element(By.XPATH, \"//div[contains(@class, 'education-list')]/ul/li/p\").text\n",
    "            except Exception:\n",
    "                education = ''\n",
    "            description = driver.find_element(By.XPATH, \"//div[contains(@class, 'description')]/p\").text.replace(';', ',')\n",
    "            \n",
    "            f.write(f\";{location};{created_at};{response_time};{last_order};{'|'.join(languages)};{'|'.join(linked_acc)};{'|'.join(skills)};{education};{description}\\n\")\n",
    "            time.sleep(5)\n",
    "            driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e24632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = get_start_index()\n",
    "# end = start + 100\n",
    "# scrape_profils(subject, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf203533",
   "metadata": {},
   "source": [
    "Formatage du Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97099ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_CORRESPONDING = {\n",
    "    'janv.': '01',\n",
    "    'f√©vr.': '02',\n",
    "    'mars': '03',\n",
    "    'avr.': '04',\n",
    "    'mai': '05',\n",
    "    'juin':'06',\n",
    "    'juil.': '07',\n",
    "    'ao√ªt': '08',\n",
    "    'sept.': '09',\n",
    "    'oct.': '10',\n",
    "    'nov.': '11',\n",
    "    'd√©c.': '12',\n",
    "}\n",
    "regex = re.compile('(\\w+.?) (\\d+)')\n",
    "\n",
    "def sigmoid(x,weight=1):\n",
    "    return 1 / (1 + math.exp(-x/weight))\n",
    "\n",
    "filtre_stopfr = lambda text: [token for token in text if token.lower() not in set(stopwords.words('french'))]\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f897ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossessing(row: pd.Series):\n",
    "    \n",
    "    ## FROM skills TO skill_1, skill_2, ...\n",
    "    try:\n",
    "        skills = row[\"skills\"].split(\"|\")\n",
    "    except AttributeError:\n",
    "        skills = []\n",
    "    \n",
    "    for i, skill in enumerate(skills):\n",
    "        row[f\"skill_{i+1}\"] = skill\n",
    "    \n",
    "    ## FROM linked_accs TO linked_acc_1, linked_acc_2, ...\n",
    "    try:\n",
    "        linked_accs = row[\"linked_acc\"].split(\"|\")\n",
    "    except AttributeError:\n",
    "        linked_accs = []\n",
    "    \n",
    "    for i, linked_acc in enumerate(linked_accs):\n",
    "        row[f\"linked_acc_{i+1}\"] = linked_acc\n",
    "    \n",
    "    ## FROM languages TO language_1, language_2, ...\n",
    "    ## ADD level_1, level_2, ...\n",
    "    try:\n",
    "        languages = row[\"languages\"].split(\"|\")\n",
    "    except AttributeError:\n",
    "        languages = []\n",
    "        \n",
    "    languages = [tuple(language.split('-', 1)) for language in languages]\n",
    "    \n",
    "    for i, (language, level) in enumerate(languages):\n",
    "        if '/' in level: level = level.split('/')[0].strip()\n",
    "        row[f\"language_{i+1}\"] = language\n",
    "        row[f\"level_{i+1}\"] = level\n",
    "\n",
    "    ## FROM \"mai-2005\" TO 2005/05/01\n",
    "    row[\"created_at\"] = re.sub(regex, lambda x: f\"{x.group(2)}-{MONTH_CORRESPONDING[x.group(1)]}\", row[\"created_at\"])\n",
    "        \n",
    "    ## FROM response_time \"3 heures\" TO response_time_day, response_time_hour\n",
    "    response_time = re.match(r'(\\d+) (\\w+)', row[\"response_time\"]).groups()\n",
    "    if 'jour' in response_time[1]:\n",
    "        row[\"response_time_day\"] = response_time[0]\n",
    "        row[\"response_time_hour\"] = 0\n",
    "    else:\n",
    "        row[\"response_time_day\"] = 0\n",
    "        row[\"response_time_hour\"] = response_time[0]\n",
    "    \n",
    "    ## FROM last_order \"3 jours\" TO last_order_day, last_order_hour\n",
    "    last_order = re.match(r'(\\d+) (\\w+)', row[\"last_order\"]).groups()\n",
    "    if 'jour' in last_order[1]:\n",
    "        row[\"last_order_day\"] = last_order[0]\n",
    "        row[\"last_order_hour\"] = 0\n",
    "    else:\n",
    "        row[\"last_order_day\"] = 0\n",
    "        row[\"last_order_hour\"] = last_order[0]\n",
    "    \n",
    "    ## ADD score \n",
    "    row['score'] = row['note'] * sigmoid(row['nb_comments'], 50)\n",
    "    \n",
    "    ## Drop columns\n",
    "    row.drop([\"languages\", \"linked_acc\", \"skills\", \"response_time\", \"last_order\"], inplace=True)\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ea26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "    df[\"last_order_day\"] = df[\"last_order_day\"].astype('int')\n",
    "    df[\"last_order_hour\"] = df[\"last_order_hour\"].astype('int')\n",
    "    df[\"response_time_hour\"] = df[\"response_time_hour\"].astype('int')\n",
    "    df[\"response_time_day\"] = df[\"response_time_day\"].astype('int')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23883186",
   "metadata": {},
   "outputs": [],
   "source": [
    "profils = pd.read_csv(f'./data/{subject}/profils.csv', sep=';', names=PROFILS_COLUMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630f0ab",
   "metadata": {},
   "source": [
    "Analyse des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a2cf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_skill(df: pd.DataFrame):\n",
    "    skills = Counter()\n",
    "    for rowIndex, row in df.filter(regex=r'skill_\\d+').iterrows():\n",
    "        for columnIndex, value in row.items():\n",
    "            if not pd.isnull(value): skills[value] += 1\n",
    "\n",
    "    return skills.most_common(10)\n",
    "\n",
    "def handle_languages(df: pd.DataFrame):\n",
    "    languages = Counter()\n",
    "    for rowIndex, row in df.filter(regex=r'language_\\d+').iterrows():\n",
    "        for columnIndex, value in row.items():\n",
    "            if not pd.isnull(value): languages[value] += 1\n",
    "\n",
    "    return languages.most_common(10)\n",
    "\n",
    "def handle_note(df: pd.DataFrame):\n",
    "    print(pd.cut(df['note'], bins=np.linspace(0, 5, num=50)))\n",
    "\n",
    "def get_best(df: pd.DataFrame, nb: int) -> pd.DataFrame:\n",
    "    return df.sort_values(by='score', ascending=False).iloc[:nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16669e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = FrenchStemmer()\n",
    "\n",
    "    # remove accents\n",
    "    text = unidecode.unidecode(str(text))\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = \" \".join(tokenizer.tokenize(text))\n",
    "    \n",
    "    # remove useless words\n",
    "    words = filtre_stopfr(word_tokenize(text, language=\"french\"))\n",
    "    \n",
    "    # normalize words\n",
    "    words = \" \".join([stemmer.stem(w) for w in words])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5658b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_div(df: pd.DataFrame, category: str):\n",
    "\n",
    "    skills = handle_skill(df)\n",
    "    languages = handle_languages(df)\n",
    "    profils = get_best(df, 3)\n",
    "\n",
    "    category_title = html.H2(category.title(), style={'font-size': '1.7rem', 'text-align': 'center'})\n",
    "\n",
    "    best_profils_title = html.H3('üí™ Meilleurs profils üí™', style={'font-size': '1.5rem', 'text-align': 'center'})\n",
    "\n",
    "    graph_skills = html.Div(dcc.Graph(\n",
    "        figure=px.bar(df, x=[i[0] for i in skills], y=[i[1] for i in skills], barmode=\"group\", title='Comp√©tences', labels={'x': '', 'y': ''})\n",
    "    ))\n",
    "\n",
    "    graph_languages = html.Div(dcc.Graph(\n",
    "        figure=px.bar(df, x=[i[0] for i in languages], y=[i[1] for i in languages], barmode=\"group\", title='Langues', labels={'x': '', 'y': ''})\n",
    "    ))\n",
    "\n",
    "\n",
    "    txt = normalize_text(' '.join(df['description'].to_list()))\n",
    "    wordcloud.generate(txt)\n",
    "    wordcloud.to_file(filename='images/words.png')\n",
    "    \n",
    "    encoded_image = base64.b64encode(open('images/words.png', 'rb').read())\n",
    "    image_words = html.Div(html.Img(src=f'data:image/png;base64,{encoded_image.decode()}'), style={'margin': '20px 0'})\n",
    "\n",
    "    profils_div = []\n",
    "    for index, profil in profils.iterrows():\n",
    "        \n",
    "        div = html.Div([\n",
    "            html.Span(profil['name'], style={'font-size': '1.2rem'}),\n",
    "            html.Span(f\"‚≠ê {profil['note']} ({profil['nb_comments']})\", style={'margin': '5px 0'}),\n",
    "            html.Span(f\"üèÜ Score : {round(profil['score'], 3)}\", style={'margin': '5px 0'}),\n",
    "            html.Span(profil['description']),\n",
    "            html.A('Lien du profil', href=profil['profil_link'], style={'font-size': '1.2rem'})\n",
    "        ], style={'margin': '0 20px', 'padding': '10px', 'border': '1px solid white', 'width': '400px', 'display': 'flex', 'flex-direction': 'column', 'align-items': 'center'})\n",
    "\n",
    "        profils_div.append(div)\n",
    "\n",
    "    profils_render = html.Div(profils_div, style={'display': 'flex'})\n",
    "\n",
    "    return html.Div(children=[\n",
    "        category_title,\n",
    "        best_profils_title,\n",
    "        profils_render,\n",
    "        image_words,\n",
    "        graph_skills,\n",
    "        graph_languages,\n",
    "    ], style={'color': 'white', 'display': 'flex', 'flex-direction': 'column', 'align-items': 'center'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40016d",
   "metadata": {},
   "source": [
    "Je cr√©e un serveur Dash pour visualiser certaines informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f07bf5a5-14ac-43e1-b08a-eed019efa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309d2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['javascript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8472d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for category in categories:\n",
    "\n",
    "    profils = pd.read_csv(f'./data/{category}/profils.csv', sep=';', names=PROFILS_COLUMS)\n",
    "    \n",
    "    new_profils = (profils\n",
    "        .merge(cards, how='left', on=\"name\") \n",
    "        .apply(preprossessing, axis=1) \n",
    "    )\n",
    "    new_profils = convert_type_column(new_profils)\n",
    "\n",
    "    dfs.append(new_profils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da77228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      (4.898, 5.0]\n",
      "1      (4.898, 5.0]\n",
      "2      (4.898, 5.0]\n",
      "3      (4.898, 5.0]\n",
      "4      (4.898, 5.0]\n",
      "5      (4.898, 5.0]\n",
      "6      (4.898, 5.0]\n",
      "7      (4.898, 5.0]\n",
      "8      (4.898, 5.0]\n",
      "9      (4.898, 5.0]\n",
      "10     (4.898, 5.0]\n",
      "11     (4.898, 5.0]\n",
      "12     (4.898, 5.0]\n",
      "13     (4.898, 5.0]\n",
      "14     (4.898, 5.0]\n",
      "15     (4.898, 5.0]\n",
      "16     (4.898, 5.0]\n",
      "17     (4.898, 5.0]\n",
      "18     (4.898, 5.0]\n",
      "19     (4.898, 5.0]\n",
      "20     (4.898, 5.0]\n",
      "21     (4.898, 5.0]\n",
      "22     (4.898, 5.0]\n",
      "23     (4.898, 5.0]\n",
      "24     (4.898, 5.0]\n",
      "25     (4.898, 5.0]\n",
      "26     (4.898, 5.0]\n",
      "27     (4.898, 5.0]\n",
      "28     (4.898, 5.0]\n",
      "29     (4.898, 5.0]\n",
      "30     (4.898, 5.0]\n",
      "31     (4.898, 5.0]\n",
      "32    (4.388, 4.49]\n",
      "33     (4.898, 5.0]\n",
      "34     (4.898, 5.0]\n",
      "35     (4.898, 5.0]\n",
      "36     (4.898, 5.0]\n",
      "37     (4.898, 5.0]\n",
      "38     (4.898, 5.0]\n",
      "39     (4.898, 5.0]\n",
      "40     (4.898, 5.0]\n",
      "41     (4.898, 5.0]\n",
      "42     (4.898, 5.0]\n",
      "43     (4.898, 5.0]\n",
      "44     (4.898, 5.0]\n",
      "45     (4.898, 5.0]\n",
      "46     (4.898, 5.0]\n",
      "47     (4.898, 5.0]\n",
      "48     (4.898, 5.0]\n",
      "Name: note, dtype: category\n",
      "Categories (49, interval[float64, right]): [(0.0, 0.102] < (0.102, 0.204] < (0.204, 0.306] < (0.306, 0.408] ... (4.592, 4.694] < (4.694, 4.796] < (4.796, 4.898] < (4.898, 5.0]]\n"
     ]
    }
   ],
   "source": [
    "handle_note(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9d34b37-4e4a-41b6-a67b-e0e8bc059759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     NaN\n",
      "1     NaN\n",
      "2     NaN\n",
      "3     NaN\n",
      "4     NaN\n",
      "5     NaN\n",
      "6     NaN\n",
      "7     NaN\n",
      "8     NaN\n",
      "9     NaN\n",
      "10    NaN\n",
      "11    NaN\n",
      "12    NaN\n",
      "13    NaN\n",
      "14    NaN\n",
      "15    NaN\n",
      "16    NaN\n",
      "17    NaN\n",
      "18    NaN\n",
      "19    NaN\n",
      "20    NaN\n",
      "21    NaN\n",
      "22    NaN\n",
      "23    NaN\n",
      "24    NaN\n",
      "25    NaN\n",
      "26    NaN\n",
      "27    NaN\n",
      "28    NaN\n",
      "29    NaN\n",
      "30    NaN\n",
      "31    NaN\n",
      "32    NaN\n",
      "33    NaN\n",
      "34    NaN\n",
      "35    NaN\n",
      "36    NaN\n",
      "37    NaN\n",
      "38    NaN\n",
      "39    NaN\n",
      "40    NaN\n",
      "41    NaN\n",
      "42    NaN\n",
      "43    NaN\n",
      "44    NaN\n",
      "45    NaN\n",
      "46    NaN\n",
      "47    NaN\n",
      "48    NaN\n",
      "Name: note, dtype: category\n",
      "Categories (0, interval[int64, right]): []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff7f2676cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @app.callback(\n",
    "#     Output('dd-output-container', 'children'),\n",
    "#     Input('demo-dropdown', 'value'),\n",
    "# )\n",
    "# def update_output(value: str):\n",
    "#     return f'{value.title()}'\n",
    "\n",
    "app.layout = html.Div(children=[get_div(df, category) for df, category in zip(dfs, categories)])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(\n",
    "        debug=True,\n",
    "        mode='inline'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
